{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing required libraries\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "#Decision tree classifier from the sklearn library.\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "#Logistic Regression from the sklearn library.\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "#GridSearchCV\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecisionTreeModel:\n",
    "\n",
    "    def __init__(self, df):\n",
    "        self.df = df\n",
    "\n",
    "    def model(self, X_train_pad, Y_train_f, X_val_pad, Y_val_f):\n",
    "\n",
    "        clf = DecisionTreeClassifier(criterion = 'entropy')\n",
    "        \n",
    "        #Training the decision tree classifier. \n",
    "        clf.fit(X_train_pad, Y_train_f)\n",
    "\n",
    "        #Predicting labels on the test set.\n",
    "        y_pred =  clf.predict(X_val_pad)\n",
    "\n",
    "\n",
    "        x = ('Accuracy Score on train data: ', accuracy_score(y_true=Y_train_f, y_pred=clf.predict(X_train_pad)))\n",
    "        y = ('Accuracy Score on test data: ', accuracy_score(y_true=Y_val_f, y_pred=y_pred))\n",
    "\n",
    "        return (x, y)\n",
    "\n",
    "\n",
    "    def modelHp(self, X_train_pad, Y_train_f, X_val_pad, Y_val_f):\n",
    "        clf = DecisionTreeClassifier()\n",
    "        param_dict = {\n",
    "            'criterion': ['gini', 'entropy'],\n",
    "            'max_depth': range(20,30),\n",
    "            #min_samples_leaf is also used to control over-fitting by defining that each leaf has more than one element.\n",
    "            'min_samples_leaf': range(7,21)\n",
    "        }\n",
    "\n",
    "        grid = GridSearchCV(clf,\n",
    "                            param_grid = param_dict,\n",
    "                            cv = 10,\n",
    "                            n_jobs = -1)\n",
    "\n",
    "        #Training the decision tree classifier. \n",
    "        grid.fit(X_train_pad, Y_train_f)\n",
    "\n",
    "        #\n",
    "        grid.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticRegressionModel:\n",
    "\n",
    "    def __init__(self, df):\n",
    "        self.df = df\n",
    "\n",
    "    def model(self, X_train_pad, ytrain, X_val_pad, ytest):\n",
    "        clf = LogisticRegression()\n",
    "\n",
    "        #Training the logistic regression classifier. \n",
    "        clf.fit(X_train_pad, ytrain)\n",
    "\n",
    "        #Predicting labels on the test set.\n",
    "        y_pred =  clf.predict(X_val_pad)\n",
    "\n",
    "        x = ('Accuracy Score on train data: ', accuracy_score(y_true=ytrain, y_pred=clf.predict(X_train_pad)))\n",
    "        y = ('Accuracy Score on test data: ', accuracy_score(y_true=ytest, y_pred=y_pred))\n",
    "        return (x, y)\n",
    "\n",
    "    \n",
    "    def modelHp(self, X_train_pad, ytrain, X_val_pad, ytest):\n",
    "        clf = LogisticRegression()\n",
    "        \n",
    "        param_dict = {\n",
    "            #A regression model that uses L1 regularization technique is called Lasso Regression and model which uses L2 is called  Ridge Regression\n",
    "            'penalty': ['l1', 'l2'],\n",
    "            #use paramter C as our regularization parameter.Parameter C = 1/λ\n",
    "            #Lambda (λ) controls the trade-off between allowing the model to increase it's complexity as much as it wants with trying to keep it simple.\n",
    "            'C': [0.001, .009, 0.01, .09, 1, 5, 10, 25]\n",
    "        }\n",
    "\n",
    "        grid = GridSearchCV(clf,\n",
    "                            param_grid = param_dict,\n",
    "                            cv = 10,\n",
    "                            n_jobs = -1)\n",
    "\n",
    "        #Training the decision tree classifier. \n",
    "        grid.fit(X_train_pad, ytrain)\n",
    "\n",
    "        #Predicting labels on the test set.\n",
    "        y_pred =  grid.predict(X_val_pad)\n",
    "\n",
    "        x = ('Accuracy Score on train data: ', accuracy_score(y_true=ytrain, y_pred=grid.predict(X_train_pad)))\n",
    "        y = ('Accuracy Score on test data: ', accuracy_score(y_true=ytest, y_pred=y_pred))\n",
    "        return (x, y)\n",
    "           \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "         "
   ]
  }
 ]
}